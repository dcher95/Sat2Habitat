{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset of multiple text descriptions for each image index. Additional column that has wikipedia columns for random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "sys.path.append('/scratch/cher/Sat2Habitat/data_prep/geocell') \n",
    "# from gbif_utils import _clean_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_key_path = '/scratch/cher/Sat2Habitat/data/grid_key_0.01deg.csv'\n",
    "habitat_info_path = '/scratch/cher/Sat2Habitat/data/occurrence.txt'\n",
    "species_wiki_path = '/scratch/cher/Sat2Habitat/data/species_wiki.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_key_df = pd.read_csv(grid_key_path)\n",
    "\n",
    "species_wiki_df = pd.read_csv(species_wiki_path)\n",
    "species_wiki_df.rename(columns={col: f\"{col}_wiki\" if col != 'species' else col for col in species_wiki_df.columns}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_720315/1592039262.py:2: DtypeWarning: Columns (49,56,57,59,60,61,63,64,65,66,67,68,73,74,77,78,79,83,89,91,92,95,97,98,99,100,101,102,103,105,106,107,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,135,137,138,140,141,142,143,144,145,146,147,149,150,151,152,153,154,155,160,162,163,164,167,168,171,173,174,177,178,182,183,184,185,186,189,190,191,192,193,194,195,196,197,198,199,200,204,208,209,210,211,220,221) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  habitat_info = pd.read_csv(habitat_info_path, sep=\"\\t\", on_bad_lines='skip')\n",
      "/scratch/cher/Sat2Habitat/data_prep/geocell/gbif_utils.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['stateProvince'] = data['stateProvince'].replace({'New jersey' : 'New Jersey',\n"
     ]
    }
   ],
   "source": [
    "# clean occurrences\n",
    "habitat_info = pd.read_csv(habitat_info_path, sep=\"\\t\", on_bad_lines='skip')\n",
    "habitat_info = habitat_info[~habitat_info['habitat'].isna()].copy()\n",
    "habitat_info = _clean_data(habitat_info)\n",
    "\n",
    "habitat_info['lat'] = gpd.GeoDataFrame(habitat_info, geometry='geometry').centroid.y\n",
    "habitat_info['lon'] = gpd.GeoDataFrame(habitat_info, geometry='geometry').centroid.x\n",
    "\n",
    "text_cleaned = habitat_info[['occurrenceID', 'species' ,'habitat']].copy()\n",
    "text_cleaned_w_geo = habitat_info[['occurrenceID', 'species' ,'habitat', 'lat', 'lon']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['habitat', 'habitat_wiki', 'distribution and habitat_wiki', 'description_wiki', 'ecology_wiki', 'distribution_wiki', 'header_wiki']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all occurrences within remote image grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overhead_image_to_text_df_grouped(grid_key_df, text_cleaned, species_wiki_df, text_cols):\n",
    "    '''\n",
    "    Add species wiki info.\n",
    "    Merge dataframes on 'occurrenceID' to ensure we have all habitat text information\n",
    "    Group by occurrenceID to combine all habitat text\n",
    "    '''\n",
    "    text_w_wiki = text_cleaned.merge(species_wiki_df, on='species', how='left')\n",
    "    merged_df = pd.merge(grid_key_df, text_w_wiki, on=\"occurrenceID\")\n",
    "\n",
    "    grouped_data = []\n",
    "    for col in text_cols:\n",
    "        merged_df[col] = merged_df[col].fillna('').astype(str)\n",
    "        grouped_col = merged_df.groupby(\"key\")[col].apply(' '.join).reset_index()\n",
    "        grouped_col[col] = grouped_col[col].str.strip()\n",
    "        grouped_col.rename(columns={col: f\"grouped_{col}\"}, inplace=True)\n",
    "        grouped_data.append(grouped_col)\n",
    "\n",
    "    grouped_df = grouped_data[0]\n",
    "    for df in grouped_data[1:]:\n",
    "        grouped_df = pd.merge(grouped_df, df, on=\"key\", how=\"left\")\n",
    "    \n",
    "    return grouped_df\n",
    "\n",
    "grouped_df = overhead_image_to_text_df_grouped(grid_key_df, text_cleaned, species_wiki_df, text_cols)\n",
    "grouped_df.to_csv('/scratch/cher/Sat2Habitat/data/gridkey2text_grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Crevices in bark of red oak // On humus on rock ledge by headwaters // Damp, shaded pockets in schist bluff // Rock bottom of fast-flowing stream // By headwaters // wet soil along stream'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example!\n",
    "grouped_df['grouped_text'][908]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each occurrence seperately, but uses grid which has consistent remote images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_w_geo_wiki = text_cleaned_w_geo.merge(species_wiki_df, on='species', how='left')\n",
    "merged_df_w_geo = pd.merge(grid_key_df, text_w_geo_wiki, on=\"occurrenceID\")\n",
    "\n",
    "merged_df_w_geo.to_csv('/scratch/cher/Sat2Habitat/data/gridkey2text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BandAid: Make Train and Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/data/cher/Sat2Habitat/data/crisp/train_10.csv')\n",
    "val = pd.read_csv('/data/cher/Sat2Habitat/data/crisp/val_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_ims = [i.replace('.jpg', '') for i in os.listdir('/data/cher/Sat2Habitat/data/bing_train_10p/')]\n",
    "val_ims = [i.replace('.jpg', '') for i in os.listdir('/data/cher/Sat2Habitat/data/bing_train_10p/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append train and val dataframes\n",
    "combined_df = pd.concat([train, val])\n",
    "\n",
    "# Subset the combined dataframe to only include 'key' in train_ims and val_ims\n",
    "new_train_df = combined_df[combined_df['key'].astype(str).isin(train_ims)]\n",
    "new_val_df = combined_df[combined_df['key'].astype(str).isin(val_ims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_val_df.to_csv('/data/cher/Sat2Habitat/data/crisp/val_10-tst.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clippatch)",
   "language": "python",
   "name": "clippatch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
