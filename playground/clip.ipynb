{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## CLIP using multi habitat sentences per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from crop_image import getImages\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision import transforms\n",
    "\n",
    "# TODO: Change to be dimensions of continental US.\n",
    "# bounds = [-90.6809899999999942, -90.0909899999996924, 38.4560099999999991, 38.8860099999999136]\n",
    "\n",
    "class MultiData(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.coords = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.obs = self.coords.drop_duplicates(subset=[\"species\"])[\"species\"].tolist()\n",
    "        self.obs = list(sorted(self.obs))\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "        self.stl_coords = pd.read_csv(\"st_louis_coords.csv\")\n",
    "        self.spec_freqs = self.coords.value_counts(['species']) / self.coords.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        lon = float(self.coords.iloc[idx][\"decimalLongitude\"])\n",
    "        lat = float(self.coords.iloc[idx][\"decimalLatitude\"])\n",
    "        feats_lon = 2 * ((lon - bounds[0]) / (bounds[1] - bounds[0])) - 1\n",
    "        feats_lat = 2 * ((lat - bounds[2]) / (bounds[3] - bounds[2])) - 1\n",
    "        feats = torch.FloatTensor([np.sin(np.pi*feats_lon/2), np.cos(np.pi*feats_lon/2+np.pi/2), np.sin(np.pi*feats_lat/2), np.cos(np.pi*feats_lat/2+np.pi/2)])\n",
    "        image = Image.fromarray(getImages(lon, lat, self.stl_coords))\n",
    "        image = self.image_processor(image, return_tensors=\"pt\")\n",
    "        rand_lon = np.random.uniform(bounds[0]+0.01, bounds[1]-0.01)\n",
    "        rand_lat = np.random.uniform(bounds[2]+0.01, bounds[3]-0.01)\n",
    "        rand_feats_lon = 2 * ((rand_lon - bounds[0]) / (bounds[1] - bounds[0])) - 1\n",
    "        rand_feats_lat = 2 * ((rand_lat - bounds[2]) / (bounds[3] - bounds[2])) - 1\n",
    "        rand_feats = torch.FloatTensor([np.sin(np.pi*rand_feats_lon/2), np.cos(np.pi*rand_feats_lon/2+np.pi/2), np.sin(np.pi*rand_feats_lat/2), np.cos(np.pi*rand_feats_lat/2+np.pi/2)])\n",
    "        rand_image = Image.fromarray(getImages(rand_lon, rand_lat, self.stl_coords))\n",
    "        rand_image = self.image_processor(rand_image, return_tensors=\"pt\")\n",
    "        species = self.coords.iloc[idx][\"species\"]\n",
    "        species_class = self.obs.index(species)\n",
    "        species_weights = 1 / (self.spec_freqs[species] + 1e-5)\n",
    "\n",
    "        return image, torch.LongTensor([species_class]), feats, rand_image, rand_feats, species_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "File: crisp.py\n",
    "------------------\n",
    "Our implementation of CLIP classes and functions for CRISP. \n",
    "Uses openclip code. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- Model class ----------------- #\n",
    "\n",
    "VALID_ENCODERS = [\"resnet50\"]\n",
    "\n",
    "class CrispModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Trainable PyTorch module for CLIP/CRISP pre-training. \n",
    "    Has two submodules:\n",
    "    - sat2cap encoder 1 \n",
    "    - CLIP text encoder 2\n",
    "    User must pass in a PyTorch encoder module for each submodule. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_name, embedding_dim=512, pretrained_weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # assert encoder name is valid\n",
    "        assert encoder_name in VALID_ENCODERS, f\"encoder name {encoder_name} is not valid. Valid encoders are {VALID_ENCODERS}\"\n",
    "    \n",
    "        # construct the encoder modules\n",
    "        self.ground_description_encoder = self.construct_encoder(encoder_name, embedding_dim, pretrained_weights)\n",
    "        self.remote_sensing_encoder = self.construct_encoder(encoder_name, embedding_dim, pretrained_weights)\n",
    "\n",
    "        # extra \n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # see https://github.com/mlfoundations/open_clip/blob/6ee59e10510ec9761b8b9871b9fd1eeb8e28627d/src/open_clip/model.py#L202\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "\n",
    "    def construct_encoder(self, encoder_name, embedding_dim, pretrained_weights=None):\n",
    "        \"\"\"\n",
    "        Construct the encoder module. \n",
    "        Specify the encoder name and optionally pass in pretrained weights\n",
    "        name from torchvision docs. \n",
    "\n",
    "        scratched: \n",
    "            base_model = torchvision.models.resnet50(weights=pretrained_weights)\n",
    "            base_layers = list(base_model.children())[:-1]\n",
    "            projection_layer = torch.nn.Linear(base_model.fc.in_features, embedding_dim)\n",
    "            modules = base_layers + [projection_layer]\n",
    "            encoder = torch.nn.Sequential(*modules)\n",
    "            return encoder\n",
    "        \"\"\"\n",
    "        if encoder_name == \"resnet50\":\n",
    "            model = torchvision.models.resnet50(weights=pretrained_weights)\n",
    "            d = model.fc.in_features\n",
    "            model.fc = nn.Linear(d, embedding_dim)\n",
    "            return model\n",
    "        \n",
    "\n",
    "    def lock(self, encoder_to_lock):\n",
    "        \"\"\"\n",
    "        Lock certain parameters of the model so that it cannot be trained.\n",
    "        See CoCa paper and https://github.com/mlfoundations/open_clip/blob/\n",
    "        6ee59e10510ec9761b8b9871b9fd1eeb8e28627d/src/open_clip/modified_resnet.py#L154.  \n",
    "        \"\"\"\n",
    "        if encoder_to_lock == \"ground_description\":\n",
    "            # freeze the ground image encoder parameters\n",
    "            for param in self.ground_description_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif encoder_to_lock == \"remote_sensing\":\n",
    "            # freeze the remote sensing encoder parameters\n",
    "            for param in self.remote_sensing_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            raise ValueError(f\"encoder_to_lock must be either 'ground_description' or 'remote_sensing'\")\n",
    "        \n",
    "\n",
    "    def encode_remote_sensing_image(self, x):\n",
    "        return self.remote_sensing_encoder(x)\n",
    "    \n",
    "\n",
    "    def encode_ground_description(self, x):\n",
    "        return self.ground_description_encoder(x)\n",
    "\n",
    "\n",
    "    def load_remote_sensing_encoder_weights(self, encoder_path):\n",
    "        \"\"\"\n",
    "        If you have weights for the remote sensing encoder, load them here. \n",
    "        \"\"\"\n",
    "        self.remote_sensing_encoder.load_state_dict(torch.load(encoder_path))\n",
    "        print(f\"Successfully loaded remote sensing encoder weights from {encoder_path}\")\n",
    "\n",
    "    \n",
    "    def load_ground_description_encoder_weights(self, encoder_path):\n",
    "        \"\"\"\n",
    "        If you have weights for the ground image encoder, load them here. \n",
    "        \"\"\"\n",
    "        self.ground_description_encoder.load_state_dict(torch.load(encoder_path))\n",
    "        print(f\"Successfully loaded ground image encoder weights from {encoder_path}\")\n",
    "\n",
    "\n",
    "    def cosine_similarity_logits(self, ground_description, remote_sensing_image):\n",
    "        \"\"\"\n",
    "        Maps images into latent space and computes cosine similarity between them as the logits. \n",
    "        We also have this in the loss. \n",
    "        Gets the CLIP matrix (one for each image input)\n",
    "        \"\"\"\n",
    "        # see https://github.com/openai/CLIP/blob/a9b1bf5920416aaeaec965c25dd9e8f98c864f16/clip/model.py#LL362C9-L362C9. \n",
    "        \n",
    "        # get latent features \n",
    "        ground_description_latent = self.encode_ground_description(ground_description)\n",
    "        remote_sensing_latent = self.encode_remote_sensing_image(remote_sensing_image)\n",
    "        \n",
    "        # normalized features\n",
    "        ground_description_latent = ground_description_latent / ground_description_latent.norm(dim=1, keepdim=True)\n",
    "        remote_sensing_latent = remote_sensing_latent / remote_sensing_latent.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_ground_description = logit_scale * ground_description_latent @ remote_sensing_latent.t()\n",
    "        logits_per_remote_sensing_image = logits_per_ground_description.t()\n",
    "\n",
    "        return logits_per_ground_description, logits_per_remote_sensing_image\n",
    "    \n",
    "\n",
    "    def alternate_cosine_similarity_logits(self, ground_description, remote_sensing_image):\n",
    "        \"\"\"\n",
    "        for debugging. the first version uses openai official repo code.\n",
    "        This uses https://github.com/mlfoundations/open_clip/blob/6ee59e10510ec9761b8b9871b9fd1eeb8e28627d/src/open_clip/loss.py#L102. \n",
    "        Note: after some debugging, this function returns the same value \n",
    "        (Pdb) cos_sim_1\n",
    "        (tensor([[-0.7614]], grad_fn=<MmBackward0>), tensor([[-0.7614]], grad_fn=<TBackward0>))\n",
    "        (Pdb) cos_sim_1.shape\n",
    "        *** AttributeError: 'tuple' object has no attribute 'shape'\n",
    "        (Pdb) cos_sim_2\n",
    "        (tensor([[-0.7614]], grad_fn=<MmBackward0>), tensor([[-0.7614]], grad_fn=<MmBackward0>))\n",
    "        (Pdb) cos_sim_1 == cos_sim_2\n",
    "        True\n",
    "        \"\"\"\n",
    "        # get latent features \n",
    "        ground_description_latent = self.encode_ground_description(ground_description)\n",
    "        remote_sensing_latent = self.encode_remote_sensing_image(remote_sensing_image)\n",
    "        \n",
    "        # normalized features\n",
    "        ground_description_latent = ground_description_latent / ground_description_latent.norm(dim=1, keepdim=True)\n",
    "        remote_sensing_latent = remote_sensing_latent / remote_sensing_latent.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_ground_description = logit_scale * ground_description_latent @ remote_sensing_latent.T\n",
    "        logits_per_remote_sensing_image = logit_scale * remote_sensing_latent @ ground_description_latent.T\n",
    "        \n",
    "        return logits_per_ground_description, logits_per_remote_sensing_image\n",
    "    \n",
    "    \n",
    "    def forward(self, ground_description, remote_sensing_image):\n",
    "        \"\"\"\n",
    "        Forward pass through the model. Produces cosine sim logits. \n",
    "        \"\"\"\n",
    "        gr_logits, rs_logits = self.cosine_similarity_logits(ground_description, remote_sensing_image)\n",
    "        return gr_logits, rs_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Loss class ----------------- #\n",
    "\n",
    "\n",
    "class ClipLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_ground_truth(self, device, num_logits) -> torch.Tensor:\n",
    "        # calculated ground-truth\n",
    "        return torch.arange(num_logits, device=device, dtype=torch.long)\n",
    "        # my confusion is that this will be [0,1,..., batch-size] but why this the label? \n",
    "        # I think its cuz you are prediciting which cosine sim score goes with which image-image pair in the batch. \n",
    "        # that way, you are optimizing for the diagonal of the matrix to be the highest since i == j (positive pair). \n",
    "\n",
    "\n",
    "    def forward(self, logits_per_ground_description, logits_per_remote_sensing_image):\n",
    "        device = logits_per_ground_description.device\n",
    "\n",
    "        labels = self.get_ground_truth(device, logits_per_ground_description.shape[0])\n",
    "\n",
    "        total_loss = (\n",
    "            F.cross_entropy(logits_per_ground_description, labels) +\n",
    "            F.cross_entropy(logits_per_remote_sensing_image, labels)\n",
    "        ) / 2\n",
    "\n",
    "        return total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cvglobal)",
   "language": "python",
   "name": "cvglobal"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
