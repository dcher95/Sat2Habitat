{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cher/miniconda3/envs/clippatch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "import math\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import PretrainedConfig\n",
    "import open_clip\n",
    "# import clip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from taxabind import TaxaBind\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "IMAGE_PATH = \"/data/cher/Sat2Habitat/data/naip\"\n",
    "CSV_PATH = \"/data/cher/Sat2Habitat/data/gridkey2text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, preprocss = clip.load(\"ViT-B/16\", device=device)\n",
    "config = PretrainedConfig.from_pretrained(\"MVRL/taxabind-config\")\n",
    "taxabind = TaxaBind(config)\n",
    "sat_encoder = taxabind.get_sat_encoder()\n",
    "location_encoder = taxabind.get_location_encoder()\n",
    "# text_encoder = taxabind.get_text_encoder()\n",
    "text_encoder = taxabind.get_text_encoder()\n",
    "tokenizer = taxabind.get_tokenizer()\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CSV_PATH)\n",
    "# remove_ids = set()\n",
    "# with open (\"remove.txt\", \"r\") as f:\n",
    "#     for line in f:\n",
    "#         inat_id = line.split('_')[0]\n",
    "#         remove_ids.add(int(inat_id))\n",
    "\n",
    "# filtered_data = data[~data[\"inat_id\"].isin(remove_ids)]\n",
    "# filtered_data.to_csv(\"filtered_data.csv\", index=False)\n",
    "\n",
    "# sat_id = data['key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiData(Dataset):\n",
    "    def __init__(self, image_path, csv_path, transform=None):\n",
    "        self.image_path = Path(image_path)\n",
    "        self.csv_path = csv_path\n",
    "        self.image_dict = self._build_image_dict()\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        # _, self.occ_id = pd.factorize(self.data[\"occurrenceID\"])\n",
    "        # self.occ_id = self.occ_id.tolist() # ??\n",
    "\n",
    "        # text params\n",
    "        self.hab_desc = 'habitat'\n",
    "        self.alt_cols = ['habitat_wiki', 'distribution and habitat_wiki', 'description_wiki', 'ecology_wiki', 'distribution_wiki', 'header_wiki']\n",
    "        self.random_prob = 0.9\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        sat_id = row[\"key\"]\n",
    "        # observer = row[\"rights_holder\"]\n",
    "        # observer_id = torch.tensor(self.observer_id.index(observer))\n",
    "        lat = torch.tensor(row[\"lat\"])\n",
    "        lon = torch.tensor(row[\"lon\"])\n",
    "        image_file = self.image_dict.get(sat_id)\n",
    "        if image_file:\n",
    "            image = Image.open(image_file)\n",
    "            image_transform = transforms.Compose([\n",
    "                    transforms.Resize((256, 256)),\n",
    "                    transforms.RandomCrop((224, 224)),\n",
    "                    transforms.RandomHorizontalFlip(0.5),\n",
    "                    transforms.GaussianBlur(5, (0.01, 1.0)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            image = image_transform(image).to(device)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No image found for sat_id: {sat_id}\")\n",
    "        \n",
    "        # Get the text description (habitat or randomized)\n",
    "        text = self._get_text_randomized(row)\n",
    "        \n",
    "        return image, text, torch.tensor([lat, lon])\n",
    "    \n",
    "    def _build_image_dict(self):\n",
    "        image_dict = {}\n",
    "        for image_file in self.image_path.glob(\"*.png\"):\n",
    "            try:\n",
    "                sat_id = image_file.stem.split(\"/\")[-1].replace(\".png\" , \"\")\n",
    "                image_dict[sat_id] = image_file\n",
    "            except ValueError:\n",
    "                print(f\"Invalid image file name {image_file}\")\n",
    "        return image_dict\n",
    "    \n",
    "    def _get_text_randomized(self, row):\n",
    "\n",
    "        if np.random.rand() < self.random_prob:\n",
    "            return row[self.hab_desc]\n",
    "        else:\n",
    "            \n",
    "            alternative_values = row[self.alt_cols].to_numpy()\n",
    "            non_nan_values = alternative_values[pd.notna(alternative_values)]\n",
    "            \n",
    "            # If there are non-NaN values, select one randomly\n",
    "            if non_nan_values.size > 0:\n",
    "                return np.random.choice(non_nan_values)\n",
    "            \n",
    "            # If all alternatives are NaN, return 'habitat' as a fallback\n",
    "            return row[self.hab_desc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, sat_encoder, location_encoder, text_encoder, tokenizer, output_dim=512):\n",
    "        super().__init__()\n",
    "        # self.clip_model = clip_model\n",
    "        self.sat_encoder = sat_encoder\n",
    "        self.location_encoder = location_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.tokenizer = tokenizer  \n",
    "\n",
    "        # Projection layers for consistent output dimensions\n",
    "        # self.image_projection = nn.Linear(sat_encoder_output_dim, output_dim)\n",
    "        # self.text_projection = nn.Linear(text_encoder_output_dim, output_dim)\n",
    "    \n",
    "    def forward(self, image, lat_long, text_tokens):\n",
    "        image_features = self.sat_encoder(image)\n",
    "        # image_features = self.image_projection(image_features)\n",
    "\n",
    "        lat_long_features = self.location_encoder(lat_long.float())\n",
    "\n",
    "        text_features = self.text_encoder.encode_text(text_tokens)\n",
    "        # text_features = self.text_projection(text_features)\n",
    "\n",
    "        combined_features = image_features.image_embeds + lat_long_features\n",
    "        return torch.nn.functional.normalize(combined_features, dim=-1), torch.nn.functional.normalize(text_features, dim=-1)\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MultiData(IMAGE_PATH, CSV_PATH)\n",
    "\n",
    "train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices) \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True) # 64 works on 1 gpu. Use 2 gpus for 512??\n",
    "validate_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "model = ContrastiveModel(sat_encoder, location_encoder, text_encoder, tokenizer).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "contrastive_loss = torch.nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losst at 0: 0.9733008742332458\n",
      "Losst at 10: 0.020391780883073807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 17\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLosst at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        image, text, lat_long = batch\n",
    "        image = image.to(device)\n",
    "        lat_long = lat_long.to(device)\n",
    "        text_tokens = tokenizer(text).to(device)\n",
    "\n",
    "        combined_features, text_features = model(image, lat_long, text_tokens)\n",
    "        loss = contrastive_loss(combined_features, text_features, torch.ones(combined_features.size(0)).to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Losst at {i}: {loss.item()}\")\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for image, text, lat_long in validate_dataloader:\n",
    "            image = image.to(device)\n",
    "            lat_long = lat_long.to(device)\n",
    "            text_tokens = tokenizer(text).to(device)\n",
    "\n",
    "            combined_features, text_features = model(image, lat_long, text_tokens)\n",
    "            loss = contrastive_loss(combined_features, text_features, torch.ones(combined_features.size(0)).to(device))\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(validate_dataloader)\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clippatch)",
   "language": "python",
   "name": "clippatch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
